import datetime
import io
import re
from collections import Counter

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

st.set_page_config(page_title="æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè§£æï¼ˆå¯¾è©±å¼ï¼‰", page_icon="ğŸ“Š")
st.title("ğŸ“Š æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè§£æãƒ„ãƒ¼ãƒ«ï¼ˆãƒãƒ£ãƒƒãƒˆå¼ï¼‰")
st.write(
    """
    CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€Œãƒãƒ£ãƒƒãƒˆå½¢å¼ã€ã§
    ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã«ã¤ã„ã¦è³ªå•ã§ãã¾ã™ï¼ˆä¾‹: ã‚«ãƒ©ãƒ ä¸€è¦§ã€ç‰¹å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®æ¤œç´¢ã€çµ±è¨ˆè¦ç´„ãªã©ï¼‰ã€‚
    æœŸå¾…ã•ã‚Œã‚‹CSVã®ã‚«ãƒ©ãƒ ï¼ˆãƒ˜ãƒƒãƒ€ï¼‰ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™:
    ç•ªå·,å­¦ç”Ÿç•ªå·,ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”,æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,å›ç­”æ—¥æ™‚
    """
)

# -------------------------
# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–
# -------------------------
uploaded_file = st.file_uploader(
    "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆUTF-8ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
    type=["csv"],
    help="ãƒ˜ãƒƒãƒ€è¡Œ: ç•ªå·,å­¦ç”Ÿç•ªå·,ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”,æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,å›ç­”æ—¥æ™‚",
)

# ã‚µãƒ³ãƒ—ãƒ«CSVï¼ˆè¡¨ç¤ºç”¨ï¼‰
sample_csv = """ç•ªå·,å­¦ç”Ÿç•ªå·,ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”,æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,å›ç­”æ—¥æ™‚
1,S053,ã“ã®æˆæ¥­ã§å¾®ç©åˆ†ã«å¯¾ã™ã‚‹ç†è§£ãŒæ·±ã¾ã£ãŸã€‚ç‰¹ã«æ¼”ç¿’å•é¡ŒãŒè‰¯ã‹ã£ãŸã€‚,5,3,2025-10-25 10:05:12
2,S012,æ¿æ›¸ãŒå°‘ã—æ—©ãã¦ã¤ã„ã¦ã„ãã®ãŒå¤§å¤‰ã ã£ãŸãŒã€å†…å®¹ã¯ã¨ã¦ã‚‚ãŸã‚ã«ãªã£ãŸã€‚,4,4,2025-10-25 10:11:34
3,S076,åŸºæœ¬ã‹ã‚‰ä¸å¯§ã«æ•™ãˆã¦ãã‚Œã¦åˆ†ã‹ã‚Šã‚„ã™ã‹ã£ãŸã€‚å¿œç”¨å•é¡Œã«ã‚‚ã£ã¨æŒ‘æˆ¦ã—ãŸã„ã€‚,5,2,2025-10-25 10:18:55
4,S009,æ­£ç›´ã€å°‘ã—é€€å±ˆã ã£ãŸã€‚ã‚‚ã†å°‘ã—å®Ÿç”Ÿæ´»ã¨ã®é–¢é€£ã‚’èª¬æ˜ã—ã¦ã»ã—ã‹ã£ãŸã€‚,2,3,2025-10-25 10:25:01
5,S034,å…ˆç”Ÿã®èª¬æ˜ãŒè«–ç†çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚æ•°å­¦ã®æ¥½ã—ã•ãŒå°‘ã—åˆ†ã‹ã£ãŸæ°—ãŒã™ã‚‹ã€‚,5,3,2025-10-25 10:32:40
6,S022,èª²é¡Œã®é‡ãŒå¤šãã¦è² æ‹…ã ã£ãŸãŒã€ãã®åˆ†åŠ›ãŒã¤ã„ãŸã¨æ€ã†ã€‚,4,5,2025-10-25 10:38:09
7,S061,ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¯ãƒ¼ã‚¯ãŒæ¥½ã—ã‹ã£ãŸã€‚ä»–ã®å­¦ç”Ÿã¨è­°è«–ã™ã‚‹ã“ã¨ã§ç†è§£ãŒæ·±ã¾ã£ãŸã€‚,5,3,2025-10-25 10:44:17
8,S002,äºˆç¿’ãŒå¿…é ˆã ã¨æ„Ÿã˜ãŸã€‚ã¤ã„ã¦ã„ããŸã‚ã«ã‹ãªã‚ŠåŠªåŠ›ãŒå¿…è¦ã ã£ãŸã€‚,3,5,2025-10-25 10:50:22
9,S073,æˆæ¥­ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚‚ã¡ã‚‡ã†ã©è‰¯ãã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãŒæ˜ç¢ºã ã£ãŸã€‚,4,2,2025-10-25 10:57:38
10,S030,æ•™ç§‘æ›¸é€šã‚Šã®å†…å®¹ã ã£ãŸãŒã€è§£èª¬ãŒä¸å¯§ã§ç†è§£ã—ã‚„ã™ã‹ã£ãŸã€‚,4,3,2025-10-25 11:03:00
"""

def load_csv(file) -> pd.DataFrame:
    if isinstance(file, str):
        buf = io.StringIO(file)
        df = pd.read_csv(buf)
    else:
        try:
            df = pd.read_csv(file)
        except Exception:
            file.seek(0)
            df = pd.read_csv(io.TextIOWrapper(file, encoding="utf-8"))
    return df

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    col_map = {}
    cols = list(df.columns)
    for c in cols:
        c_norm = re.sub(r"\s+", "", str(c)).lower()
        if "ç•ªå·" in c or c_norm in ("id", "number"):
            col_map[c] = "ç•ªå·"
        elif "å­¦ç”Ÿ" in c or "student" in c_norm:
            col_map[c] = "å­¦ç”Ÿç•ªå·"
        elif "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ" in c or "answer" in c_norm or "comment" in c_norm:
            col_map[c] = "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"
        elif "å½¹ç«‹" in c or "help" in c_norm:
            col_map[c] = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        elif "é›£ã—" in c or "difficult" in c_norm:
            col_map[c] = "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        elif "æ—¥æ™‚" in c or "date" in c_norm:
            col_map[c] = "å›ç­”æ—¥æ™‚"
    return df.rename(columns=col_map)

def make_columns_unique(df: pd.DataFrame) -> pd.DataFrame:
    cols = list(df.columns)
    seen = {}
    new_cols = []
    for c in cols:
        if c not in seen:
            seen[c] = 1
            new_cols.append(c)
        else:
            seen[c] += 1
            new_name = f"{c}({seen[c]-1})"
            while new_name in seen:
                seen[c] += 1
                new_name = f"{c}({seen[c]-1})"
            seen[new_name] = 1
            new_cols.append(new_name)
    df.columns = new_cols
    return df

def extract_top_words(series: pd.Series, top_n=10):
    texts = series.dropna().astype(str).tolist()
    tokens = []
    for t in texts:
        t_clean = re.sub(r"[^\w\u3000-\u303F\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF]", " ", t)
        for w in t_clean.split():
            if len(w) >= 2:
                tokens.append(w)
    counter = Counter(tokens)
    return counter.most_common(top_n)

def make_safe_preview(df: pd.DataFrame, n=50) -> pd.DataFrame:
    df_preview = df.head(n).copy()
    for col in df_preview.columns:
        try:
            if df_preview[col].dropna().apply(lambda x: isinstance(x, datetime.date) and not isinstance(x, datetime.datetime)).any():
                df_preview[col] = pd.to_datetime(df_preview[col], errors="coerce")
        except Exception:
            pass
    for col in df_preview.columns:
        try:
            has_bad = df_preview[col].dropna().apply(lambda x: isinstance(x, (list, dict, set, tuple))).any()
        except Exception:
            has_bad = False
        if has_bad:
            df_preview[col] = df_preview[col].astype(str)
    for col in df_preview.select_dtypes(include=["object"]).columns:
        try:
            df_preview[col] = df_preview[col].astype(str)
        except Exception:
            df_preview[col] = df_preview[col].apply(lambda x: str(x) if pd.notna(x) else x)
    return df_preview

# èª­ã¿è¾¼ã¿
if uploaded_file is not None:
    try:
        df = load_csv(uploaded_file)
    except Exception as e:
        st.error(f"CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()
    st.success("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
else:
    st.info("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã§ãã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚")
    df = load_csv(sample_csv)

# æ­£è¦åŒ–ã¨ä¸€æ„åŒ–
df = normalize_columns(df)
df = make_columns_unique(df)

# æ¬ ã‘ã¦ã„ã‚‹æƒ³å®šã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯æ³¨æ„
required_cols = [
    "ç•ªå·",
    "å­¦ç”Ÿç•ªå·",
    "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”",
    "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰",
    "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰",
    "å›ç­”æ—¥æ™‚",
]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    st.warning(f"ä»¥ä¸‹ã®æƒ³å®šã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}ã€‚å¯èƒ½ãªé™ã‚Šå­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã§è§£æã—ã¾ã™ã€‚")

# DataFrame ã®å‹æ•´å‚™
if "å›ç­”æ—¥æ™‚" in df.columns:
    try:
        df["å›ç­”æ—¥æ™‚"] = pd.to_datetime(df["å›ç­”æ—¥æ™‚"], errors="coerce")
    except Exception:
        pass
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
    df["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"] = pd.to_numeric(df["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"], errors="coerce")
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
    df["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"] = pd.to_numeric(df["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"], errors="coerce")

# -------------------------
# ãƒãƒ£ãƒƒãƒˆUI: ä¸€é€£ã®å¯¾è©±ã§è³ªå•ã§ãã‚‹ä»•çµ„ã¿
# -------------------------
st.sidebar.header("ãƒãƒ£ãƒƒãƒˆå¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹")
st.sidebar.write("ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«åŸºã¥ã„ã¦å¿œç­”ã—ã¾ã™ã€‚")

# ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ä¿æŒ
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # list of (user, bot)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_input = st.sidebar.text_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹: ã‚«ãƒ©ãƒ ä¸€è¦§ã€ã‚µãƒ³ãƒ—ãƒ«è¡Œã€'æ¼”ç¿’' ã‚’å«ã‚€è¡Œãªã©ï¼‰", value="")

# è¿½åŠ ã®ãƒœã‚¿ãƒ³ã§ã‚ˆãã‚ã‚‹è³ªå•ã‚’æŒ¿å…¥ã§ãã‚‹
if st.sidebar.button("ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’è¡¨ç¤º"):
    user_input = "ã‚«ãƒ©ãƒ ä¸€è¦§"
if st.sidebar.button("ã‚µãƒ³ãƒ—ãƒ«è¡Œã‚’è¡¨ç¤º"):
    user_input = "ã‚µãƒ³ãƒ—ãƒ«è¡Œ"
if st.sidebar.button("å½¹ç«‹ã£ãŸã‹ã®å¹³å‡"):
    user_input = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰ã‚’æ•™ãˆã¦"
if st.sidebar.button("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ï¼‰"):
    user_input = "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« 'æ¼”ç¿’' ã‚’å«ã‚€è¡Œã‚’è¡¨ç¤º"

# æ¤œç´¢å¯¾è±¡åˆ—ã‚’é¸ã¶ UIï¼ˆãƒãƒ£ãƒƒãƒˆä»¥å¤–ã§ã‚‚å˜ç‹¬ã§åˆ©ç”¨å¯èƒ½ï¼‰
st.sidebar.markdown("---")
st.sidebar.subheader("åˆ—é¸æŠï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«ä½¿ç”¨ï¼‰")
string_cols = list(df.select_dtypes(include=["object", "string"]).columns)
cols_for_search = string_cols if string_cols else list(df.columns)
search_column = st.sidebar.selectbox("æ¤œç´¢å¯¾è±¡åˆ—", options=cols_for_search, index=0 if cols_for_search else None)
st.sidebar.caption("ãƒãƒ£ãƒƒãƒˆã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ã—ãŸã„å ´åˆã€ã“ã“ã§åˆ—ã‚’é¸ã‚“ã§ã‹ã‚‰è³ªå•ã—ã¦ãã ã•ã„ã€‚")

# å‡¦ç†é–¢æ•°: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è‡ªç”±ãªãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã—ã¦ç°¡æ˜“ãƒ«ãƒ¼ãƒ«ã§å¿œç­”ã‚’ç”Ÿæˆ
def answer_query(query: str, df: pd.DataFrame) -> str:
    q = query.strip().lower()
    if q == "":
        return "è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä½•ã‹è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ã‚«ãƒ©ãƒ ä¸€è¦§ã€ã‚µãƒ³ãƒ—ãƒ«è¡Œã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹ç‰¹å®šèªã®æ¤œç´¢ãªã©ï¼‰ã€‚"

    # ã‚«ãƒ©ãƒ ä¸€è¦§
    if "ã‚«ãƒ©ãƒ " in q or "åˆ—" in q or "columns" in q:
        return "ã‚«ãƒ©ãƒ ä¸€è¦§: " + ", ".join([str(c) for c in df.columns.tolist()])

    # ã‚µãƒ³ãƒ—ãƒ«è¡Œ
    if "ã‚µãƒ³ãƒ—ãƒ«" in q or "å…ˆé ­" in q or "head" in q:
        n = 5
        m = re.search(r"(\d+)", q)
        if m:
            n = int(m.group(1))
        preview = df.head(n)
        return f"å…ˆé ­ {n} è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n\n{preview.to_string(index=False)}"

    # ç‰¹å®šã‚«ãƒ©ãƒ ã®å¹³å‡å€¤ï¼ˆå˜ç´”ãƒãƒƒãƒï¼‰
    if "å¹³å‡" in q and ("å½¹ç«‹" in q or "å½¹ã«" in q or "useful" in q):
        col = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        if col in df.columns:
            avg = df[col].mean(skipna=True)
            return f"'{col}' ã®å¹³å‡: {avg:.2f}" if not np.isnan(avg) else f"'{col}' ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        else:
            return f"åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    if "å¹³å‡" in q and ("é›£ã—" in q or "é›£ã—ã„" in q or "difficult" in q):
        col = "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        if col in df.columns:
            avg = df[col].mean(skipna=True)
            return f"'{col}' ã®å¹³å‡: {avg:.2f}" if not np.isnan(avg) else f"'{col}' ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        else:
            return f"åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®è‡ªç„¶æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: 'ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« æ¼”ç¿’ ã‚’å«ã‚€è¡Œ'ï¼‰
    m = re.search(r"(å«ã‚€|å«ã‚ã‚‹|å«ã¾ã‚Œã‚‹).{0,10}['\"â€œâ€]?([^'\"ã€\s]+)['\"â€]?", query)
    if m:
        keyword = m.group(2)
        col = search_column if search_column else "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"
        if col not in df.columns:
            return f"æ¤œç´¢å¯¾è±¡ã®åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»£æ›¿ã®åˆ—ã‚’é¸ã¶ã‹ã€ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        mask = df[col].astype(str).str.contains(keyword, case=False, na=False)
        matched = df[mask]
        if len(matched) == 0:
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆåˆ—: {col}ï¼‰ã€‚"
        else:
            # è¡¨ç¤ºã¯æœ€å¤§20è¡Œã¾ã§
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹ {len(matched)} ä»¶ã®è¡Œï¼ˆæœ€å¤§20ä»¶è¡¨ç¤ºï¼‰:\n\n{matched.head(20).to_string(index=False)}"

    # å˜ç´”ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆï¼ˆ"æ¼”ç¿’" ãªã©ï¼‰
    m2 = re.search(r"['\"â€œâ€]?([^'\"ã€\s]{2,})['\"â€]?$", query)
    if m2 and len(query.split()) == 1:
        keyword = m2.group(1)
        col = search_column if search_column else "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"
        if col in df.columns:
            mask = df[col].astype(str).str.contains(keyword, case=False, na=False)
            matched = df[mask]
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œ: {len(matched)} ä»¶ï¼ˆåˆ—: {col}ï¼‰ã€‚å…ˆé ­5ä»¶:\n\n{matched.head(5).to_string(index=False)}" if len(matched) > 0 else f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    # è©•ä¾¡åˆ†å¸ƒã‚„åŸºæœ¬çµ±è¨ˆã®è¦æ±‚
    if "åˆ†å¸ƒ" in q or "ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ " in q:
        parts = []
        if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
            vc = df["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().sort_index()
            parts.append("å½¹ç«‹ã£ãŸã‹ï¼ˆè©•ä¾¡ï¼‰:\n" + vc.to_string())
        if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
            vc2 = df["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().sort_index()
            parts.append("é›£ã—ã‹ã£ãŸã‹ï¼ˆè©•ä¾¡ï¼‰:\n" + vc2.to_string())
        return "\n\n".join(parts) if parts else "è©²å½“ã™ã‚‹è©•ä¾¡åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"

    # ãã‚Œä»¥å¤–ã¯è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ï¼ˆä»»æ„ã®åˆ—ã‚’æ¨ªæ–­ï¼‰
    # query ã«å«ã¾ã‚Œã‚‹èªã‚’ dataframe å…¨ä½“ã§æ¢ã™ï¼ˆæœ€å¤§100è¡Œè¡¨ç¤ºï¼‰
    tokens = re.findall(r"\w+|[^\s]", query)
    keyword = query.strip()
    if len(keyword) >= 1:
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’ä½¿ã£ã¦æ¤œç´¢
        text_cols = list(df.select_dtypes(include=["object", "string"]).columns)
        if not text_cols:
            return "ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…·ä½“çš„ã«ã©ã®åˆ—ã‚’æ¤œç´¢ã—ãŸã„ã‹æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        mask = pd.Series(False, index=df.index)
        for c in text_cols:
            mask = mask | df[c].astype(str).str.contains(keyword, case=False, na=False)
        matched = df[mask]
        if len(matched) == 0:
            return f"ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’æ¨ªæ–­æ¤œç´¢ï¼‰ã€‚"
        return f"ãƒ†ã‚­ã‚¹ãƒˆåˆ—æ¨ªæ–­æ¤œç´¢ã§ {len(matched)} ä»¶ãƒ’ãƒƒãƒˆï¼ˆæœ€å¤§100è¡Œè¡¨ç¤ºï¼‰:\n\n{matched.head(100).to_string(index=False)}"

    return "ã™ã¿ã¾ã›ã‚“ã€ãã®è³ªå•ã«ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚'ã‚«ãƒ©ãƒ ä¸€è¦§' ã‚„ 'ã‚µãƒ³ãƒ—ãƒ«è¡Œ'ã€'ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« æ¼”ç¿’ ã‚’å«ã‚€è¡Œ' ãªã©ã®ä¾‹ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸã‚‰å‡¦ç†ã—ã¦å±¥æ­´ã«è¿½åŠ 
if user_input:
    user_question = user_input.strip()
    st.session_state.chat_history.append(("user", user_question))
    response = answer_query(user_question, df)
    st.session_state.chat_history.append(("bot", response))

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
st.subheader("ãƒãƒ£ãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦è³ªå•")
for role, text in st.session_state.chat_history[::-1]:
    if role == "user":
        st.markdown(f"**ã‚ãªãŸ:** {text}")
    else:
        st.markdown(f"**ãƒ„ãƒ¼ãƒ«:**\n```\n{text}\n```")

# -------------------------
# æ—¢å­˜ã®è§£ææ©Ÿèƒ½ï¼ˆè¡¨ç¤ºä¸­ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãã‚°ãƒ©ãƒ•ç­‰ï¼‰
# -------------------------
st.header("è§£æãƒ‘ãƒãƒ«ï¼ˆè¡¨ç¤ºä¸­ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰")

# ç¾åœ¨ã®ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆãƒãƒ£ãƒƒãƒˆã§æ¤œç´¢ã—ã¦ matched ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ãªã‚‰ df_filtered ã‚’ä½¿ã†ï¼‰
# ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ«ã«ãƒãƒƒãƒã—ã¦ last response ã« matched DataFrame ã‚’è¿”ã™å ´åˆã€ç¾åœ¨ã¯ text å‡ºåŠ›ã®ã¿ãªã®ã§
# ã“ã“ã§ã¯ãƒ•ã‚£ãƒ«ã‚¿ç„¡ã—ã®å…¨ä½“è¡¨ç¤ºã‚’è¡Œã†ã€‚å¿…è¦ãªã‚‰ãƒãƒ£ãƒƒãƒˆå´ã§ df_filtered ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«å…¥ã‚Œã‚‹æ‹¡å¼µãŒå¯èƒ½ã€‚
df_filtered = df.copy()

col1, col2, col3 = st.columns(3)
total_responses = len(df_filtered)
col1.metric("å›ç­”æ•°ï¼ˆå…¨ä½“ï¼‰", total_responses)
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    avg_useful = df_filtered["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].mean(skipna=True)
    col2.metric("æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", f"{avg_useful:.2f}" if not np.isnan(avg_useful) else "N/A")
else:
    col2.metric("æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", "N/A")
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    avg_diff = df_filtered["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].mean(skipna=True)
    col3.metric("æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", f"{avg_diff:.2f}" if not np.isnan(avg_diff) else "N/A")
else:
    col3.metric("æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", "N/A")

st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå®‰å…¨åŒ–ã—ã¦æœ€å¤§50è¡Œï¼‰")
df_preview = make_safe_preview(df_filtered, n=50)
try:
    st.dataframe(df_preview, use_container_width=True)
except Exception as e:
    st.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    fallback = df_preview.copy()
    fallback.columns = [str(c) for c in fallback.columns]
    try:
        st.write(fallback.astype(str))
    except Exception as e2:
        st.error(f"è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e2}")
        st.write("åˆ—åä¸€è¦§:", list(df.columns))

st.subheader("è©•ä¾¡ã®åˆ†å¸ƒï¼ˆè¡¨ç¤ºä¸­ãƒ‡ãƒ¼ã‚¿ï¼‰")
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    useful_counts = df_filtered["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().reset_index()
    useful_counts.columns = ["è©•ä¾¡", "ä»¶æ•°"]
    useful_counts["è©•ä¾¡"] = useful_counts["è©•ä¾¡"].astype(str)
    chart1 = alt.Chart(useful_counts).mark_bar().encode(
        x=alt.X("è©•ä¾¡:N", title="è©•ä¾¡ï¼ˆå½¹ç«‹ã£ãŸã‹ï¼‰"),
        y=alt.Y("ä»¶æ•°:Q", title="ä»¶æ•°"),
        color=alt.Color("è©•ä¾¡:N")
    )
    st.altair_chart(chart1, use_container_width=True)
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    diff_counts = df_filtered["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().reset_index()
    diff_counts.columns = ["è©•ä¾¡", "ä»¶æ•°"]
    diff_counts["è©•ä¾¡"] = diff_counts["è©•ä¾¡"].astype(str)
    chart2 = alt.Chart(diff_counts).mark_bar().encode(
        x=alt.X("è©•ä¾¡:N", title="è©•ä¾¡ï¼ˆé›£ã—ã‹ã£ãŸã‹ï¼‰"),
        y=alt.Y("ä»¶æ•°:Q", title="ä»¶æ•°"),
        color=alt.Color("è©•ä¾¡:N")
    )
    st.altair_chart(chart2, use_container_width=True)

st.caption("æ³¨: ãƒãƒ£ãƒƒãƒˆã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“å¿œç­”ã§ã™ã€‚ã‚ˆã‚Šè‡ªç„¶ãªå¯¾è©±ã‚„è¦ç´„ã‚’æœ›ã‚€å ´åˆã¯å¤–éƒ¨ã® NLP ãƒ¢ãƒ‡ãƒ«ï¼ˆAPIï¼‰ã‚’çµ„ã¿åˆã‚ã›ã¦ãã ã•ã„ã€‚")
