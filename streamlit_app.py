import datetime
import io
import re
from collections import Counter

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st

st.set_page_config(page_title="æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè§£æï¼ˆå¯¾è©±å¼ + Geminiï¼‰", page_icon="ğŸ“Š")
st.title("ğŸ“Š æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆè§£æãƒ„ãƒ¼ãƒ«ï¼ˆãƒãƒ£ãƒƒãƒˆ + Geminiå¯¾å¿œï¼‰")
st.write(
    """
    CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã§ãã¾ã™ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒãƒ£ãƒƒãƒˆã§ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ãŒå¯èƒ½ã§ã™ã€‚
    ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ Google Geminiï¼ˆVertex AI / Vertex SDKï¼‰ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå¿œç­”ã‚’æœ‰åŠ¹ã«ã§ãã¾ã™ã€‚
    ï¼ˆGemini ã‚’ä½¿ã†å ´åˆã¯ Google Cloud ã®è¨­å®šï¼èªè¨¼ãŒå¿…è¦ã§ã™ â€” ä¸‹ã®èª¬æ˜ã‚’å‚ç…§ã—ã¦ãã ã•ã„ï¼‰
    """
)

# -------------------------
# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ»æ­£è¦åŒ–
# -------------------------
uploaded_file = st.file_uploader(
    "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼ˆUTF-8ã€ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰",
    type=["csv"],
    help="ãƒ˜ãƒƒãƒ€è¡Œ: ç•ªå·,å­¦ç”Ÿç•ªå·,ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”,æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,å›ç­”æ—¥æ™‚",
)

# ã‚µãƒ³ãƒ—ãƒ«CSVï¼ˆè¡¨ç¤ºç”¨ï¼‰
sample_csv = """ç•ªå·,å­¦ç”Ÿç•ªå·,ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”,æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰,å›ç­”æ—¥æ™‚
1,S053,ã“ã®æˆæ¥­ã§å¾®ç©åˆ†ã«å¯¾ã™ã‚‹ç†è§£ãŒæ·±ã¾ã£ãŸã€‚ç‰¹ã«æ¼”ç¿’å•é¡ŒãŒè‰¯ã‹ã£ãŸã€‚,5,3,2025-10-25 10:05:12
2,S012,æ¿æ›¸ãŒå°‘ã—æ—©ãã¦ã¤ã„ã¦ã„ãã®ãŒå¤§å¤‰ã ã£ãŸãŒã€å†…å®¹ã¯ã¨ã¦ã‚‚ãŸã‚ã«ãªã£ãŸã€‚,4,4,2025-10-25 10:11:34
3,S076,åŸºæœ¬ã‹ã‚‰ä¸å¯§ã«æ•™ãˆã¦ãã‚Œã¦åˆ†ã‹ã‚Šã‚„ã™ã‹ã£ãŸã€‚å¿œç”¨å•é¡Œã«ã‚‚ã£ã¨æŒ‘æˆ¦ã—ãŸã„ã€‚,5,2,2025-10-25 10:18:55
4,S009,æ­£ç›´ã€å°‘ã—é€€å±ˆã ã£ãŸã€‚ã‚‚ã†å°‘ã—å®Ÿç”Ÿæ´»ã¨ã®é–¢é€£ã‚’èª¬æ˜ã—ã¦ã»ã—ã‹ã£ãŸã€‚,2,3,2025-10-25 10:25:01
5,S034,å…ˆç”Ÿã®èª¬æ˜ãŒè«–ç†çš„ã§åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚æ•°å­¦ã®æ¥½ã—ã•ãŒå°‘ã—åˆ†ã‹ã£ãŸæ°—ãŒã™ã‚‹ã€‚,5,3,2025-10-25 10:32:40
6,S022,èª²é¡Œã®é‡ãŒå¤šãã¦è² æ‹…ã ã£ãŸãŒã€ãã®åˆ†åŠ›ãŒã¤ã„ãŸã¨æ€ã†ã€‚,4,5,2025-10-25 10:38:09
7,S061,ã‚°ãƒ«ãƒ¼ãƒ—ãƒ¯ãƒ¼ã‚¯ãŒæ¥½ã—ã‹ã£ãŸã€‚ä»–ã®å­¦ç”Ÿã¨è­°è«–ã™ã‚‹ã“ã¨ã§ç†è§£ãŒæ·±ã¾ã£ãŸã€‚,5,3,2025-10-25 10:44:17
8,S002,äºˆç¿’ãŒå¿…é ˆã ã¨æ„Ÿã˜ãŸã€‚ã¤ã„ã¦ã„ããŸã‚ã«ã‹ãªã‚ŠåŠªåŠ›ãŒå¿…è¦ã ã£ãŸã€‚,3,5,2025-10-25 10:50:22
9,S073,æˆæ¥­ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚‚ã¡ã‚‡ã†ã©è‰¯ãã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆãŒæ˜ç¢ºã ã£ãŸã€‚,4,2,2025-10-25 10:57:38
10,S030,æ•™ç§‘æ›¸é€šã‚Šã®å†…å®¹ã ã£ãŸãŒã€è§£èª¬ãŒä¸å¯§ã§ç†è§£ã—ã‚„ã™ã‹ã£ãŸã€‚,4,3,2025-10-25 11:03:00
"""

def load_csv(file) -> pd.DataFrame:
    if isinstance(file, str):
        buf = io.StringIO(file)
        df = pd.read_csv(buf)
    else:
        try:
            df = pd.read_csv(file)
        except Exception:
            file.seek(0)
            df = pd.read_csv(io.TextIOWrapper(file, encoding="utf-8"))
    return df

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    col_map = {}
    cols = list(df.columns)
    for c in cols:
        c_norm = re.sub(r"\s+", "", str(c)).lower()
        if "ç•ªå·" in c or c_norm in ("id", "number"):
            col_map[c] = "ç•ªå·"
        elif "å­¦ç”Ÿ" in c or "student" in c_norm:
            col_map[c] = "å­¦ç”Ÿç•ªå·"
        elif "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ" in c or "answer" in c_norm or "comment" in c_norm:
            col_map[c] = "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"
        elif "å½¹ç«‹" in c or "help" in c_norm:
            col_map[c] = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        elif "é›£ã—" in c or "difficult" in c_norm:
            col_map[c] = "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        elif "æ—¥æ™‚" in c or "date" in c_norm:
            col_map[c] = "å›ç­”æ—¥æ™‚"
    return df.rename(columns=col_map)

def make_columns_unique(df: pd.DataFrame) -> pd.DataFrame:
    cols = list(df.columns)
    seen = {}
    new_cols = []
    for c in cols:
        if c not in seen:
            seen[c] = 1
            new_cols.append(c)
        else:
            seen[c] += 1
            new_name = f"{c}({seen[c]-1})"
            while new_name in seen:
                seen[c] += 1
                new_name = f"{c}({seen[c]-1})"
            seen[new_name] = 1
            new_cols.append(new_name)
    df.columns = new_cols
    return df

def extract_top_words(series: pd.Series, top_n=10):
    texts = series.dropna().astype(str).tolist()
    tokens = []
    for t in texts:
        t_clean = re.sub(r"[^\w\u3000-\u303F\u4E00-\u9FFF\u3040-\u309F\u30A0-\u30FF]", " ", t)
        for w in t_clean.split():
            if len(w) >= 2:
                tokens.append(w)
    counter = Counter(tokens)
    return counter.most_common(top_n)

def make_safe_preview(df: pd.DataFrame, n=50) -> pd.DataFrame:
    df_preview = df.head(n).copy()
    for col in df_preview.columns:
        try:
            if df_preview[col].dropna().apply(lambda x: isinstance(x, datetime.date) and not isinstance(x, datetime.datetime)).any():
                df_preview[col] = pd.to_datetime(df_preview[col], errors="coerce")
        except Exception:
            pass
    for col in df_preview.columns:
        try:
            has_bad = df_preview[col].dropna().apply(lambda x: isinstance(x, (list, dict, set, tuple))).any()
        except Exception:
            has_bad = False
        if has_bad:
            df_preview[col] = df_preview[col].astype(str)
    for col in df_preview.select_dtypes(include=["object"]).columns:
        try:
            df_preview[col] = df_preview[col].astype(str)
        except Exception:
            df_preview[col] = df_preview[col].apply(lambda x: str(x) if pd.notna(x) else x)
    return df_preview

# èª­ã¿è¾¼ã¿
if uploaded_file is not None:
    try:
        df = load_csv(uploaded_file)
    except Exception as e:
        st.error(f"CSVã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.stop()
    st.success("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
else:
    st.info("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£æã§ãã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¾ã™ã€‚")
    df = load_csv(sample_csv)

# æ­£è¦åŒ–ã¨ä¸€æ„åŒ–
df = normalize_columns(df)
df = make_columns_unique(df)

# æ¬ ã‘ã¦ã„ã‚‹æƒ³å®šã‚«ãƒ©ãƒ ãŒã‚ã‚‹å ´åˆã¯æ³¨æ„
required_cols = [
    "ç•ªå·",
    "å­¦ç”Ÿç•ªå·",
    "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”",
    "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰",
    "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰",
    "å›ç­”æ—¥æ™‚",
]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    st.warning(f"ä»¥ä¸‹ã®æƒ³å®šã‚«ãƒ©ãƒ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}ã€‚å¯èƒ½ãªé™ã‚Šå­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã§è§£æã—ã¾ã™ã€‚")

# DataFrame ã®å‹æ•´å‚™
if "å›ç­”æ—¥æ™‚" in df.columns:
    try:
        df["å›ç­”æ—¥æ™‚"] = pd.to_datetime(df["å›ç­”æ—¥æ™‚"], errors="coerce")
    except Exception:
        pass
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
    df["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"] = pd.to_numeric(df["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"], errors="coerce")
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
    df["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"] = pd.to_numeric(df["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"], errors="coerce")

# -------------------------
# Gemini (Vertex AI) è¨­å®šï¼ˆã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼‰
# -------------------------
st.sidebar.header("LLMï¼ˆGeminiï¼‰è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
use_gemini = st.sidebar.checkbox("Gemini ã‚’ä½¿ã£ãŸè‡ªç„¶è¨€èªå¿œç­”ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=False)
st.sidebar.markdown(
    "Gemini ã‚’ä½¿ã†å ´åˆã¯ã€Vertex AI SDKï¼ˆã¾ãŸã¯ google-cloud-aiplatformï¼‰ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\n"
    "ã“ã®ã‚¢ãƒ—ãƒªã¯ã¾ãšãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸ Vertex SDK ã‚’è©¦è¡Œã—ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å¤–éƒ¨å‘¼ã³å‡ºã—ã®æ–¹æ³•ã‚’æ¡ˆå†…ã—ã¾ã™ã€‚"
)

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆãƒ­ãƒ¼ã‚«ãƒ« SDK ã‚’ä½¿ã†å ´åˆã®ãƒ¢ãƒ‡ãƒ«åã®ãƒ’ãƒ³ãƒˆï¼‰
gemini_model_hint = st.sidebar.selectbox(
    "ãƒ¢ãƒ‡ãƒ«ï¼ˆSDK/REST ã®ç’°å¢ƒã«åˆã‚ã›ã¦é¸æŠï¼‰",
    options=["gpt-4o-mini", "gemini-proto", "chat-bison@001", "text-bison@001"],
    index=2,
)

# Credential / project infoï¼ˆå¿…è¦ãªã‚‰ï¼‰
project = st.sidebar.text_input("GCP ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆï¼ˆå¿…è¦ãªå ´åˆï¼‰", value="")
location = st.sidebar.text_input("ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆä¾‹: us-central1ï¼‰", value="us-central1")

st.sidebar.markdown(
    "æ³¨æ„: Vertex SDKï¼ˆvertexai / google-cloud-aiplatformï¼‰ã‚’ä½¿ã†å ´åˆã¯ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§èªè¨¼ï¼ˆç’°å¢ƒå¤‰æ•° GOOGLE_APPLICATION_CREDENTIALS ç­‰ï¼‰ãŒå¿…è¦ã§ã™ã€‚"
)

# -------------------------
# ãƒãƒ£ãƒƒãƒˆUI: ä¸€é€£ã®å¯¾è©±ã§è³ªå•ã§ãã‚‹ä»•çµ„ã¿
# -------------------------
st.sidebar.header("ãƒãƒ£ãƒƒãƒˆå¼ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹")
st.sidebar.write("ã“ã“ã«è³ªå•ã‚’å…¥åŠ›ã™ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«åŸºã¥ã„ã¦å¿œç­”ã—ã¾ã™ã€‚Gemini ãŒæœ‰åŠ¹ãªã‚‰ LLM ã‚’å‘¼ã³å‡ºã—ã¦è‡ªç„¶è¨€èªã§å›ç­”ã—ã¾ã™ã€‚")

# ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã§ä¿æŒ
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []  # list of (user, bot)

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_input = st.sidebar.text_input("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹: ã‚«ãƒ©ãƒ ä¸€è¦§ã€ã‚µãƒ³ãƒ—ãƒ«è¡Œã€'æ¼”ç¿’' ã‚’å«ã‚€è¡Œãªã©ï¼‰", value="")

# è¿½åŠ ã®ãƒœã‚¿ãƒ³ã§ã‚ˆãã‚ã‚‹è³ªå•ã‚’æŒ¿å…¥ã§ãã‚‹
if st.sidebar.button("ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’è¡¨ç¤º"):
    user_input = "ã‚«ãƒ©ãƒ ä¸€è¦§"
if st.sidebar.button("ã‚µãƒ³ãƒ—ãƒ«è¡Œã‚’è¡¨ç¤º"):
    user_input = "ã‚µãƒ³ãƒ—ãƒ«è¡Œ"
if st.sidebar.button("å½¹ç«‹ã£ãŸã‹ã®å¹³å‡"):
    user_input = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰ã‚’æ•™ãˆã¦"
if st.sidebar.button("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ï¼‰"):
    user_input = "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« 'æ¼”ç¿’' ã‚’å«ã‚€è¡Œã‚’è¡¨ç¤º"

# æ¤œç´¢å¯¾è±¡åˆ—ã‚’é¸ã¶ UIï¼ˆãƒãƒ£ãƒƒãƒˆä»¥å¤–ã§ã‚‚å˜ç‹¬ã§åˆ©ç”¨å¯èƒ½ï¼‰
st.sidebar.markdown("---")
st.sidebar.subheader("åˆ—é¸æŠï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã«ä½¿ç”¨ï¼‰")
string_cols = list(df.select_dtypes(include=["object", "string"]).columns)
cols_for_search = string_cols if string_cols else list(df.columns)
search_column = st.sidebar.selectbox("æ¤œç´¢å¯¾è±¡åˆ—", options=cols_for_search, index=0 if cols_for_search else None)
st.sidebar.caption("ãƒãƒ£ãƒƒãƒˆã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’ã—ãŸã„å ´åˆã€ã“ã“ã§åˆ—ã‚’é¸ã‚“ã§ã‹ã‚‰è³ªå•ã—ã¦ãã ã•ã„ã€‚")

# -------------------------
# Gemini å‘¼ã³å‡ºã—ãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆå¯èƒ½ãªã‚‰ SDK ã‚’ä½¿ã†ï¼‰
# -------------------------
def call_gemini_via_vertex_sdk(prompt: str, model: str = None, max_tokens: int = 512) -> str:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã« Vertex AI ã® SDKï¼ˆvertexai / google-cloud-aiplatformï¼‰ãŒå…¥ã£ã¦ã„ã‚‹å ´åˆã«å®Ÿè¡Œã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã€‚
    - vertexai.preview.language_models.TextGenerationModel ã‚’ä½¿ã†ï¼ˆç’°å¢ƒã«ã‚ˆã‚Š import åãŒå¤‰ã‚ã‚‹ãŸã‚è©¦è¡Œã—ã¾ã™ï¼‰ã€‚
    - ã“ã“ã§ä¾‹å¤–ãŒå‡ºãŸå ´åˆã¯å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ãŸæ—¨ã‚’æ–‡å­—åˆ—ã§è¿”ã—ã¾ã™ã€‚
    """
    try:
        # æ–°ã—ã„ vertex-ai SDKï¼ˆpip install google-cloud-aiplatform ã‹ vertexaiï¼‰
        # Try vertexai (recommended modern SDK)
        try:
            import vertexai
            from vertexai.preview.language_models import TextGenerationModel

            # åˆæœŸåŒ–ã¯ project/location ãŒå¿…è¦ãªå ´åˆã«è¡Œã†
            if project:
                vertexai.init(project=project, location=location)
            model_name = model or gemini_model_hint or "chat-bison@001"
            tg = TextGenerationModel.from_pretrained(model_name)
            # ã‚·ãƒ³ãƒ—ãƒ«ãª text generationï¼ˆãƒãƒ£ãƒƒãƒˆé¢¨ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãã®ã¾ã¾æ¸¡ã™ï¼‰
            response = tg.predict(prompt, max_output_tokens=max_tokens)
            return response.text if hasattr(response, "text") else str(response)
        except Exception:
            # Fallback to google-cloud-aiplatform client (older SDK)
            from google.cloud import aiplatform

            if project:
                aiplatform.init(project=project, location=location)
            model_name = model or gemini_model_hint or "chat-bison@001"
            model = aiplatform.TextGenerationModel.from_pretrained(model_name)
            response = model.predict(prompt, max_output_tokens=max_tokens)
            # response ãŒ dict ã®ã¨ããªã©ã«å‚™ãˆã‚‹
            if hasattr(response, "text"):
                return response.text
            return str(response)
    except Exception as e:
        return f"[Gemini å‘¼ã³å‡ºã—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}]"

def call_gemini(prompt: str) -> str:
    """
    çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚Gemini ã‚’å‘¼ã¹ãªã„å ´åˆã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿”ã™ã€‚
    """
    if not use_gemini:
        return "[Gemini æœªæœ‰åŠ¹] Gemini ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã‚’ã‚ªãƒ³ã«ã—ã¦ãã ã•ã„ã€‚"
    # Try SDK-based call
    resp = call_gemini_via_vertex_sdk(prompt, model=gemini_model_hint)
    return resp

# -------------------------
# ãƒãƒ£ãƒƒãƒˆå¿œç­”ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ + Gemini ã‚’çµ„ã¿åˆã‚ã›ã‚‹ï¼‰
# -------------------------
def answer_query(query: str, df: pd.DataFrame, use_llm: bool = False) -> str:
    """
    query ã«å¯¾ã—ã¦ã¾ãšã‚·ãƒ³ãƒ—ãƒ«ãªãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹å¿œç­”ã‚’ä½œã‚‹ã€‚
    - ãƒ«ãƒ¼ãƒ«ã§å¿œç­”ãŒå¾—ã‚‰ã‚Œãªã„ã€ã¾ãŸã¯ LLM ã‚’æ˜ç¤ºçš„ã«ä½¿ã†æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯
      Gemini ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ã‚Šã€DataFrame ã®ç°¡æ˜“ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦å¿œç­”ã‚’å¾—ã‚‹ã€‚
    """
    q = query.strip()
    if q == "":
        return "è³ªå•ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä½•ã‹è³ªå•ã—ã¦ãã ã•ã„ï¼ˆä¾‹: ã‚«ãƒ©ãƒ ä¸€è¦§ã€ã‚µãƒ³ãƒ—ãƒ«è¡Œã€ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆã«å«ã¾ã‚Œã‚‹ç‰¹å®šèªã®æ¤œç´¢ãªã©ï¼‰ã€‚"

    q_lower = q.lower()

    # ã‚«ãƒ©ãƒ ä¸€è¦§
    if "ã‚«ãƒ©ãƒ " in q_lower or "åˆ—" in q_lower or "columns" in q_lower:
        return "ã‚«ãƒ©ãƒ ä¸€è¦§: " + ", ".join([str(c) for c in df.columns.tolist()])

    # ã‚µãƒ³ãƒ—ãƒ«è¡Œ
    if "ã‚µãƒ³ãƒ—ãƒ«" in q_lower or "å…ˆé ­" in q_lower or "head" in q_lower:
        n = 5
        m = re.search(r"(\d+)", q_lower)
        if m:
            n = int(m.group(1))
        preview = df.head(n)
        return f"å…ˆé ­ {n} è¡Œã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n\n{preview.to_string(index=False)}"

    # ç‰¹å®šã‚«ãƒ©ãƒ ã®å¹³å‡å€¤
    if "å¹³å‡" in q_lower and ("å½¹ç«‹" in q_lower or "å½¹ã«" in q_lower or "useful" in q_lower):
        col = "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"
        if col in df.columns:
            avg = df[col].mean(skipna=True)
            return f"'{col}' ã®å¹³å‡: {avg:.2f}" if not np.isnan(avg) else f"'{col}' ã«æ•°å€¤ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        else:
            return f"åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã®è‡ªç„¶æ–‡ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆä¾‹: 'ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« æ¼”ç¿’ ã‚’å«ã‚€è¡Œ'ï¼‰
    m = re.search(r"(å«ã‚€|å«ã‚ã‚‹|å«ã¾ã‚Œã‚‹).{0,10}['\"â€œâ€]?([^'\"ã€\s]+)['\"â€]?", q)
    if m:
        keyword = m.group(2)
        col = search_column if search_column else "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"
        if col not in df.columns:
            return f"æ¤œç´¢å¯¾è±¡ã®åˆ— '{col}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»£æ›¿ã®åˆ—ã‚’é¸ã¶ã‹ã€ã‚«ãƒ©ãƒ ä¸€è¦§ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        mask = df[col].astype(str).str.contains(keyword, case=False, na=False)
        matched = df[mask]
        if len(matched) == 0:
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆåˆ—: {col}ï¼‰ã€‚"
        else:
            return f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹ {len(matched)} ä»¶ã®è¡Œï¼ˆæœ€å¤§20ä»¶è¡¨ç¤ºï¼‰:\n\n{matched.head(20).to_string(index=False)}"

    # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§å¯¾å¿œã§ããªã„ã€ã¾ãŸã¯ LLM æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ LLM ã«å§”ã­ã‚‹
    if use_llm or ("è¦ç´„" in q_lower or "ã¾ã¨ã‚ã¦" in q_lower or "èª¬æ˜ã—ã¦" in q_lower or "è§£èª¬" in q_lower):
        # Prepare compact context: ã‚«ãƒ©ãƒ ä¸€è¦§ + ä¸Šä½10è¡Œã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ï¼‰
        context_lines = []
        context_lines.append("ã‚«ãƒ©ãƒ ä¸€è¦§: " + ", ".join([str(c) for c in df.columns.tolist()]))
        if "ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”" in df.columns:
            # include up to first 10 free-text answers for context
            text_sample = df["ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”"].dropna().astype(str).head(10).tolist()
            context_lines.append("ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã®ã‚µãƒ³ãƒ—ãƒ«:")
            for i, t in enumerate(text_sample, 1):
                context_lines.append(f"{i}. {t}")
        # also include basic stats
        stats = []
        if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
            stats.append(f"å½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰: {df['æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰'].mean(skipna=True):.2f}")
        if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df.columns:
            stats.append(f"é›£ã—ã‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰: {df['æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰'].mean(skipna=True):.2f}")
        context_lines.append(" ; ".join(stats))
        system_prompt = (
            "ã‚ãªãŸã¯æˆæ¥­ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿è§£æã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å‚ç…§ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ã‚ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚"
        )
        full_prompt = system_prompt + "\n\nã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n" + "\n".join(context_lines) + "\n\nãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: " + q
        # Call Gemini
        gemini_resp = call_gemini(full_prompt)
        return gemini_resp

    # ãã‚Œä»¥å¤–ã¯ç°¡æ˜“ãƒ†ã‚­ã‚¹ãƒˆæ¨ªæ–­æ¤œç´¢
    keyword = q.strip()
    if len(keyword) >= 1:
        text_cols = list(df.select_dtypes(include=["object", "string"]).columns)
        if not text_cols:
            return "ãƒ†ã‚­ã‚¹ãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…·ä½“çš„ã«ã©ã®åˆ—ã‚’æ¤œç´¢ã—ãŸã„ã‹æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
        mask = pd.Series(False, index=df.index)
        for c in text_cols:
            mask = mask | df[c].astype(str).str.contains(keyword, case=False, na=False)
        matched = df[mask]
        if len(matched) == 0:
            return f"ã€Œ{keyword}ã€ã«ä¸€è‡´ã™ã‚‹è¡Œã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’æ¨ªæ–­æ¤œç´¢ï¼‰ã€‚"
        return f"ãƒ†ã‚­ã‚¹ãƒˆåˆ—æ¨ªæ–­æ¤œç´¢ã§ {len(matched)} ä»¶ãƒ’ãƒƒãƒˆï¼ˆæœ€å¤§20è¡Œè¡¨ç¤ºï¼‰:\n\n{matched.head(20).to_string(index=False)}"

    return "ã™ã¿ã¾ã›ã‚“ã€ãã®è³ªå•ã«ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚'ã‚«ãƒ©ãƒ ä¸€è¦§' ã‚„ 'ã‚µãƒ³ãƒ—ãƒ«è¡Œ'ã€'ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆå›ç­”ã« æ¼”ç¿’ ã‚’å«ã‚€è¡Œ' ãªã©ã®ä¾‹ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚"

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå…¥åŠ›ã—ãŸã‚‰å‡¦ç†ã—ã¦å±¥æ­´ã«è¿½åŠ 
if user_input:
    user_question = user_input.strip()
    st.session_state.chat_history.append(("user", user_question))
    # Gemini ã‚’ä½¿ã†ã‹ã¯ use_gemini ã¾ãŸã¯ã‚¯ã‚¨ãƒªæ–‡å†…ã®æŒ‡å®šã§æ±ºã‚ã‚‹ï¼ˆã“ã“ã¯ã‚·ãƒ³ãƒ—ãƒ«ã« use_gemini ãƒ•ãƒ©ã‚°ã®ã¿ï¼‰
    response = answer_query(user_question, df, use_llm=use_gemini)
    st.session_state.chat_history.append(("bot", response))

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
st.subheader("ãƒãƒ£ãƒƒãƒˆ: ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦è³ªå•")
for role, text in st.session_state.chat_history[::-1]:
    if role == "user":
        st.markdown(f"**ã‚ãªãŸ:** {text}")
    else:
        # bot ã®è¿”ç­”ãŒè¤‡æ•°è¡Œã®å ´åˆã¯ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã§è¡¨ç¤º
        st.markdown(f"**ãƒ„ãƒ¼ãƒ«:**\n```\n{text}\n```")

# -------------------------
# è§£æãƒ‘ãƒãƒ«ï¼ˆæ—¢å­˜æ©Ÿèƒ½ï¼‰
# -------------------------
st.header("è§£æãƒ‘ãƒãƒ«ï¼ˆè¡¨ç¤ºä¸­ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰")
df_filtered = df.copy()

col1, col2, col3 = st.columns(3)
total_responses = len(df_filtered)
col1.metric("å›ç­”æ•°ï¼ˆå…¨ä½“ï¼‰", total_responses)
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    avg_useful = df_filtered["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].mean(skipna=True)
    col2.metric("æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", f"{avg_useful:.2f}" if not np.isnan(avg_useful) else "N/A")
else:
    col2.metric("æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", "N/A")
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    avg_diff = df_filtered["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].mean(skipna=True)
    col3.metric("æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", f"{avg_diff:.2f}" if not np.isnan(avg_diff) else "N/A")
else:
    col3.metric("æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆå¹³å‡ï¼‰", "N/A")

st.subheader("ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆå®‰å…¨åŒ–ã—ã¦æœ€å¤§50è¡Œï¼‰")
df_preview = make_safe_preview(df_filtered, n=50)
try:
    st.dataframe(df_preview, use_container_width=True)
except Exception as e:
    st.warning(f"ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    fallback = df_preview.copy()
    fallback.columns = [str(c) for c in fallback.columns]
    try:
        st.write(fallback.astype(str))
    except Exception as e2:
        st.error(f"è¡¨ç¤ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e2}")
        st.write("åˆ—åä¸€è¦§:", list(df.columns))

st.subheader("è©•ä¾¡ã®åˆ†å¸ƒï¼ˆè¡¨ç¤ºä¸­ãƒ‡ãƒ¼ã‚¿ï¼‰")
if "æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    useful_counts = df_filtered["æˆæ¥­ãŒå½¹ç«‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().reset_index()
    useful_counts.columns = ["è©•ä¾¡", "ä»¶æ•°"]
    useful_counts["è©•ä¾¡"] = useful_counts["è©•ä¾¡"].astype(str)
    chart1 = alt.Chart(useful_counts).mark_bar().encode(
        x=alt.X("è©•ä¾¡:N", title="è©•ä¾¡ï¼ˆå½¹ç«‹ã£ãŸã‹ï¼‰"),
        y=alt.Y("ä»¶æ•°:Q", title="ä»¶æ•°"),
        color=alt.Color("è©•ä¾¡:N")
    )
    st.altair_chart(chart1, use_container_width=True)
if "æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰" in df_filtered.columns:
    diff_counts = df_filtered["æˆæ¥­ãŒé›£ã—ã‹ã£ãŸã‹ï¼ˆï¼•æ®µéšè©•ä¾¡ï¼‰"].value_counts().reset_index()
    diff_counts.columns = ["è©•ä¾¡", "ä»¶æ•°"]
    diff_counts["è©•ä¾¡"] = diff_counts["è©•ä¾¡"].astype(str)
    chart2 = alt.Chart(diff_counts).mark_bar().encode(
        x=alt.X("è©•ä¾¡:N", title="è©•ä¾¡ï¼ˆé›£ã—ã‹ã£ãŸã‹ï¼‰"),
        y=alt.Y("ä»¶æ•°:Q", title="ä»¶æ•°"),
        color=alt.Color("è©•ä¾¡:N")
    )
    st.altair_chart(chart2, use_container_width=True)

st.caption(
    "æ³¨: Gemini å‘¼ã³å‡ºã—ã‚’è¡Œã†ã«ã¯ Vertex AI é–¢é€£ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆvertexai ãªã©ï¼‰ã‚’ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€"
    "é©åˆ‡ãªèªè¨¼ï¼ˆGOOGLE_APPLICATION_CREDENTIALS ã®è¨­å®šãªã©ï¼‰ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚"
)
